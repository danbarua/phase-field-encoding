{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neuromorphic Software: Phase-Encoding\n",
    "\n",
    "What it is\n",
    "What it does\n",
    "Explain fixed bio encoder + learnable MultiLayerPerceptron for Readout (classifier)\n"
   ],
   "id": "16bb457ff771456f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook setup\n",
    "\n",
    "Imports, Random Seeding, Device Setup - Training will take about 20 minutes if running on CPU."
   ],
   "id": "931f06d812fbd91e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:42.515407Z",
     "start_time": "2025-04-23T13:05:42.475813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ],
   "id": "caedfd34a31e6d18",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:42.798600Z",
     "start_time": "2025-04-23T13:05:42.761549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to ensure reproducibility, we should set the random seed consistently\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)  # for multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ],
   "id": "25d28495c380b35d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:42.909516Z",
     "start_time": "2025-04-23T13:05:42.896704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "dd3e83c212d88c55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment Setup",
   "id": "7a68621a2afb617d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:43.039680Z",
     "start_time": "2025-04-23T13:05:43.031909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 0.0 Define Experiment Parameters ---\n",
    "NUM_CLASSES = 10  # 10 digits in MNIST Dataset\n",
    "MAX_ROTATION_DEGREES = 35  # Rotate images by up to 35 degrees\n",
    "MAX_TRANSLATION = 0.1  # Translate images by up to 10%\n",
    "\n",
    "# --- 0.1 Define Classifier Model Parameters ---\n",
    "L1_OUTPUT_DIMS = 784\n",
    "L2_OUTPUT_DIMS = 512\n",
    "DROPOUT_PROB = 0.5  # Randomly \"mute\" a proportion of input neurons\n",
    "\n",
    "# --- 0.2 Define Training Parameters ---\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 3"
   ],
   "id": "c6464a56d93cee52",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading and Preprocessing MNIST Data\n",
    "- Defines image transformations to be applied to the training and testing data, respectively.\n",
    "- The training data is augmented with random rotations and translations.\n",
    "- Both datasets are converted to PyTorch tensors and normalized.\n",
    "- Loads the MNIST dataset.\n",
    "- The root argument specifies where to store the data, train=True indicates the training set, and download=True downloads the data if it's not already present.\n",
    "- Splits the training data into training and validation sets.\n",
    "- This is important to evaluate the model's performance during training and prevent overfitting.\n",
    "- Creates data loaders for the training, validation, and testing sets.\n",
    "- These loaders handle batching and shuffling of the data, making it easier to feed into the model during training and evaluation."
   ],
   "id": "21f5fd59c8ff5fe9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:43.361766Z",
     "start_time": "2025-04-23T13:05:43.127372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. Load and Normalize MNIST ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(MAX_ROTATION_DEGREES),\n",
    "    transforms.RandomAffine(degrees=0, translate=(MAX_TRANSLATION, MAX_TRANSLATION)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "# --- Split training data into training and validation sets ---\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# --- 2. Dataloaders ---\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "32df13d66abd6809",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Phase Encoding Setup\n",
    "\n",
    "#### Phase Encoding Parameters:\n",
    "- These variables (`N`, `omega_active`, `theta_thresh`, `omega_ref`, `n`, `kappa, x`) are parameters used in the phase encoding process.\n",
    "- They control aspects of how the image data is transformed into a phase-encoded representation.\n",
    "\n",
    "#### Input:\n",
    "\n",
    "- $I$: Flattened image (a vector of pixel values)\n",
    "- $N$: The X and Y dimensions of the image\n",
    "- $\\omega_{active}$: Active frequency parameter\n",
    "- $x$: Spatial layout parameter (a vector) initialized to a range of values representing a phase gradient across a field of sensory neurons.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- $\\theta_{thresh}$: Threshold phase (set to `0.0` in the code). When a Neuron's phase has rotated through $2\\pi$ to $0$ we consider the Neuron has generated a Spike.\n",
    "- $\\omega_{ref}$: Reference frequency (set to `20 Hz` in the code).\n",
    "- $n$ A constant (set to 4.0 in the code). TODO: What is this?\n",
    "- $\\kappa$: A constant (set to $2\\pi$ in the code). TODO: What is this?"
   ],
   "id": "23bd46916386418f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:43.412927Z",
     "start_time": "2025-04-23T13:05:43.402168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3. Phase Encoding Parameters ---\n",
    "N = 28 * 28\n",
    "omega_active = torch.ones(N, dtype=torch.float32) * 2 * np.pi * 20.0\n",
    "theta_thresh = 0.0\n",
    "omega_ref = 2 * np.pi * 8.0\n",
    "n = 4.0\n",
    "kappa = 2 * np.pi\n",
    "x = torch.linspace(0, 1, N, dtype=torch.float32)  # TODO: Check if this is overwritten in torch.NN code?\n",
    "\n",
    "EncoderFnType = Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]"
   ],
   "id": "4eab0a36bfc3d402",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Phase Encoder Function\n",
    "\n",
    "#### Calculation:\n",
    "- Initial Phase: $$\\theta_{init} = I \\cdot 2\\pi$$\n",
    "- Phase Difference: $$\\Delta\\theta = (\\theta_{thresh} - \\theta_{init} + 2\\pi) \\pmod{2\\pi}$$\n",
    "- Spike Time: $$t_{spike} = \\frac{\\Delta\\theta}{\\omega_{active}}$$\n",
    "- Reference Phase: $$\\theta_{ref} = \\left(\\frac{\\omega_{ref} \\cdot t_{spike} + \\kappa \\cdot x}{n}\\right) \\pmod{2\\pi}$$\n",
    "- Final Phase Difference: $$\\phi = (\\theta_{thresh} - \\theta_{ref} + 2\\pi) \\pmod{2\\pi}$$\n",
    "\n",
    "#### Output:\n",
    "- Encoded image (a concatenated vector of cosine and sine of the final phase difference):  $$[\\cos(\\phi), \\sin(\\phi)]$$\n",
    "\n",
    "### Formal Definition:\n",
    "$$ \\text{encode\\_image}(I, \\omega_{active}, x) = [\\cos(\\phi), \\sin(\\phi)] $$\n",
    "where\n",
    "$$ \\phi = (\\theta_{thresh} - \\theta_{ref} + 2\\pi) \\pmod{2\\pi} $$\n",
    "and $\\theta_{ref}$ is calculated as described in the steps above."
   ],
   "id": "2b680036b2e3b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:43.469838Z",
     "start_time": "2025-04-23T13:05:43.462853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4. Phase Encoder Function ---\n",
    "def encode_image(img_flat: torch.Tensor,\n",
    "                 omega_active_param: torch.Tensor,\n",
    "                 x_param: torch.Tensor) -> torch.Tensor:\n",
    "    theta_init = (img_flat * 2 * np.pi)\n",
    "    delta_theta = torch.fmod(theta_thresh - theta_init + 2 * np.pi, 2 * np.pi)\n",
    "    t_spike = delta_theta / omega_active_param\n",
    "    theta_ref = torch.fmod((omega_ref * t_spike + kappa * x_param) / n, 2 * np.pi)\n",
    "    phase_diff = torch.fmod(theta_thresh - theta_ref + 2 * np.pi, 2 * np.pi)\n",
    "    return torch.cat([torch.cos(phase_diff), torch.sin(phase_diff)])"
   ],
   "id": "a4c761e22a93107d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:43.527878Z",
     "start_time": "2025-04-23T13:05:43.516706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5. Define Classifier ---\n",
    "class PhaseClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(PhaseClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(N * 2, L1_OUTPUT_DIMS)\n",
    "        self.layer2 = nn.Linear(L1_OUTPUT_DIMS, L2_OUTPUT_DIMS)\n",
    "        self.layer3 = nn.Linear(L2_OUTPUT_DIMS, num_classes)\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,\n",
    "                batch: torch.Tensor,\n",
    "                omega_active: torch.Tensor,\n",
    "                spatial_layout: torch.Tensor,\n",
    "                encoder_fn: EncoderFnType = encode_image\n",
    "                ):\n",
    "        encoded_images: list[torch.Tensor] = []\n",
    "        for img in batch:\n",
    "            img_flat = img.view(-1)  # Flatten the image\n",
    "            encoded_img = encoder_fn(\n",
    "                img_flat,\n",
    "                omega_active,\n",
    "                spatial_layout)\n",
    "            encoded_images.append(encoded_img)\n",
    "        encoded_images_t = torch.stack(encoded_images)\n",
    "        encoded_images_t = self.dropout(encoded_images_t)\n",
    "        _x = self.relu(self.layer1(encoded_images_t))\n",
    "        _x = self.relu(self.layer2(_x))\n",
    "        logits = self.layer3(_x)\n",
    "        return logits"
   ],
   "id": "f56bf025049ca8ef",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Setup\n",
    "- Blah"
   ],
   "id": "77029a276db7cb34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:46:43.417690Z",
     "start_time": "2025-04-23T13:46:43.409773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Visualising Training Loss Over Epochs ---\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create an empty DataFrame to hold epoch and loss values\n",
    "df = pd.DataFrame({'Epoch': [], 'Loss': [], 'Validation Loss': []})\n",
    "\n",
    "\n",
    "# Function to update the Plotly Express figure\n",
    "def update_training_plot(epoch, loss, validation_loss):\n",
    "    global df\n",
    "    df = df.append({'Epoch': epoch, 'Loss': loss, 'Validation Loss': validation_loss}, ignore_index=True)\n",
    "    fig = px.line(df, x='Epoch', y=['Loss', 'Validation Loss'], title='Training Losses Over Epochs')\n",
    "    clear_output(wait=True)  # Clear previous output for real-time update effect\n",
    "    display(fig)"
   ],
   "id": "75112c6201f732b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:47:24.637008Z",
     "start_time": "2025-04-23T13:47:24.626289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6. Training Loop ---\n",
    "def run_training(train_loader: DataLoader,\n",
    "                 val_loader: DataLoader,\n",
    "                 encoder_fn: EncoderFnType = encode_image) -> nn.Module:\n",
    "    num_classes = NUM_CLASSES\n",
    "    model = PhaseClassifier(num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- Early Stopping Parameters ---\n",
    "    best_val_loss = float('inf')\n",
    "    patience = PATIENCE\n",
    "    counter = 0\n",
    "\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, omega_active.to(device), x.to(device), encoder_fn)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tqdm loop\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images, omega_active.to(device), x.to(device))\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        update_training_plot(epoch + 1, loss.item(), val_loss)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    return model"
   ],
   "id": "3dd0f9cd14202664",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:05:43.669398Z",
     "start_time": "2025-04-23T13:05:43.664314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 7. Evaluation ---\n",
    "def run_evaluation(model: nn.Module,\n",
    "                   test_loader: DataLoader,\n",
    "                   encoder_fn: EncoderFnType = encode_image):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs: torch.Tensor = model(images, omega_active.to(device), x.to(device), encoder_fn)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f}%\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training and Testing the Model\n",
    "Let's hook it all up!"
   ],
   "id": "15afca4734b83487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:27:11.715257Z",
     "start_time": "2025-04-23T13:05:43.711820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = run_training(train_loader, val_loader, encode_image)\n",
    "run_evaluation(model, test_loader, encode_image)"
   ],
   "id": "6d7068c4149b763b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.5342, Validation Loss: 1.5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 1.3446, Validation Loss: 1.2470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 1.1018, Validation Loss: 1.0600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.8272, Validation Loss: 0.8868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.6571, Validation Loss: 0.7906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.7485, Validation Loss: 0.7417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.7817, Validation Loss: 0.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.6116, Validation Loss: 0.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.5416, Validation Loss: 0.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.5430, Validation Loss: 0.6002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.4946, Validation Loss: 0.6088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.5523, Validation Loss: 0.5896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.6199, Validation Loss: 0.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.5869, Validation Loss: 0.5998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.5164, Validation Loss: 0.5799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.6186, Validation Loss: 0.5466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.3927, Validation Loss: 0.5391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.3869, Validation Loss: 0.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.4707, Validation Loss: 0.5099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.3311, Validation Loss: 0.5051\n",
      "Accuracy on the test set: 90.75%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ablation Study:\n",
    "\n",
    "We'll create a modified version of the code where we remove the `t_spike` calculation and directly encode the pixel values using cosine and sine.\n",
    "\n",
    "Comparing the performance of this modified version to our original model should further evidence the importance of the first spike time calculation."
   ],
   "id": "c65915d81540a464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:38:42.635135Z",
     "start_time": "2025-04-23T13:38:42.628897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def direct_encode(img_flat: torch.Tensor,\n",
    "                  omega_active: torch.Tensor,\n",
    "                  spatial_layout: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Directly encodes pixel values using cosine and sine functions.\n",
    "    \n",
    "    This implementation returns the same dimensions as encode_image calculated\n",
    "    as [cos(scaled_pixels), sin(scaled_pixels)] where scaled_pixels is in the range [0, 2*pi].\n",
    "\n",
    "    Args:\n",
    "      img_flat: A flattened PyTorch tensor representing the image pixels.\n",
    "      omega_active: Not used in this direct method.\n",
    "      spatial_layout: Not used in this direct method.\n",
    "\n",
    "    Returns:\n",
    "      A PyTorch tensor with shape (2*N,) corresponding to the encoded image.\n",
    "    \"\"\"\n",
    "    # Scale pixel values to the range [0, 2*pi]\n",
    "    scaled_pixels = img_flat * 2 * torch.pi\n",
    "\n",
    "    # Compute cosine and sine directly for each pixel\n",
    "    cos_encoding = torch.cos(scaled_pixels)\n",
    "    sin_encoding = torch.sin(scaled_pixels)\n",
    "\n",
    "    # Concatenate to produce an encoding with 2N dimensions\n",
    "    encoded_representation = torch.cat([cos_encoding, sin_encoding])\n",
    "\n",
    "    return encoded_representation"
   ],
   "id": "7934e8f787b3768a",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and Testing the Default Encoder\n",
   "id": "d1836a6f9af16e16"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-23T13:47:36.669398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "wrapped_default_encode = partial(direct_encode)\n",
    "model = run_training(train_loader, val_loader, wrapped_default_encode)\n",
    "run_evaluation(model, test_loader, wrapped_default_encode)"
   ],
   "id": "5bab47d1c9c4dee5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  83%|████████▎ | 620/750 [00:39<00:08, 14.88it/s, loss=1.62]"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
