{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danbarua/phase-field-encoding/blob/main/augmented_phase_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "16bb457ff771456f"
      },
      "cell_type": "markdown",
      "source": [
        "# Neuromorphic Software: Phase-Encoding\n",
        "\n",
        "Phase encoding is a biologically-inspired method of converting input data (like images) into temporal spike patterns, similar to how biological neurons process information. In neuromorphic computing, it plays a crucial role by:\n",
        "\n",
        "1. Converting static input (pixel values) into dynamic temporal patterns\n",
        "2. Mimicking the way biological sensory neurons encode information through spike timing\n",
        "3. Enabling efficient processing of information in neuromorphic hardware\n",
        "\n",
        "This implementation combines:\n",
        "- A fixed bio-inspired phase encoder that transforms input data into spike-timing based representations\n",
        "- A learnable MultiLayerPerceptron for classification that processes these phase-encoded patterns\n",
        "\n",
        "The encoder uses phase differences and spike timing to create a rich representation of the input data, while preserving spatial relationships through a reference phase calculation. This makes it particularly suitable for pattern recognition tasks like digit classification.\n"
      ],
      "id": "16bb457ff771456f"
    },
    {
      "metadata": {
        "id": "931f06d812fbd91e"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook setup\n",
        "\n",
        "Imports, Random Seeding, Device Setup - Training time varies based on hardware: approximately 20 minutes on CPU, and significantly faster on GPU.\n"
      ],
      "id": "931f06d812fbd91e"
    },
    {
      "metadata": {
        "id": "caedfd34a31e6d18"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Callable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from IPython import display\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm"
      ],
      "id": "caedfd34a31e6d18",
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "25d28495c380b35d"
      },
      "cell_type": "code",
      "source": [
        "# to ensure reproducibility, we should set the random seed consistently\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)  # for multi-GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)"
      ],
      "id": "25d28495c380b35d",
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "dd3e83c212d88c55",
        "outputId": "e7a4bb22-ac7c-463f-ecf1-a09db7b77113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# --- Device Configuration ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "id": "dd3e83c212d88c55",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "7a68621a2afb617d"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiment Setup\n",
        "\n",
        "This section defines key parameters for the experiment:\n",
        "\n",
        "- Classification task parameters (number of classes, data augmentation limits)\n",
        "- Neural network architecture parameters (layer dimensions, dropout rate)\n",
        "- Training hyperparameters (epochs, batch size, learning rate, early stopping patience)\n",
        "\n",
        "These parameters control the behavior of both the phase encoder and the classifier training process.\n"
      ],
      "id": "7a68621a2afb617d"
    },
    {
      "metadata": {
        "id": "c6464a56d93cee52"
      },
      "cell_type": "code",
      "source": [
        "# --- 0.0 Define Experiment Parameters ---\n",
        "NUM_CLASSES = 10  # 10 digits in MNIST Dataset\n",
        "MAX_ROTATION_DEGREES = 35  # Rotate images by up to 35 degrees\n",
        "MAX_TRANSLATION = 0.1  # Translate images by up to 10%\n",
        "\n",
        "# --- 0.1 Define Classifier Model Parameters ---\n",
        "L1_OUTPUT_DIMS = 784\n",
        "L2_OUTPUT_DIMS = 512\n",
        "DROPOUT_PROB = 0.5  # Randomly \"mute\" a proportion of input neurons\n",
        "\n",
        "# --- 0.2 Define Training Parameters ---\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "PATIENCE = 3"
      ],
      "id": "c6464a56d93cee52",
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "e0b8cac91851e62b"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading and Preprocessing MNIST Data\n",
        "- Defines image transformations to be applied to the training and testing data, respectively.\n",
        "- The training data is augmented with random rotations and translations.\n",
        "- Both datasets are converted to PyTorch tensors and normalized.\n",
        "- Normalization is crucial as it helps neural networks converge faster and perform better by ensuring all input features are on a similar scale and centered around zero.\n",
        "- Loads the MNIST dataset.\n",
        "- The root argument specifies where to store the data, train=True indicates the training set, and download=True downloads the data if it's not already present.\n",
        "- Splits the training data into training and validation sets.\n",
        "- This is important to evaluate the model's performance during training and prevent overfitting.\n",
        "- Creates data loaders for the training, validation, and testing sets.\n",
        "- These loaders handle batching and shuffling of the data, making it easier to feed into the model during training and evaluation.\n"
      ],
      "id": "e0b8cac91851e62b"
    },
    {
      "metadata": {
        "id": "32df13d66abd6809",
        "outputId": "01c87533-46e8-4db0-8624-1109adf9bd45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# --- 1. Load and Normalize MNIST ---\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(MAX_ROTATION_DEGREES),\n",
        "    transforms.RandomAffine(degrees=0, translate=(MAX_TRANSLATION, MAX_TRANSLATION)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "# --- Split training data into training and validation sets ---\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# --- 2. Dataloaders ---\n",
        "batch_size = BATCH_SIZE\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "32df13d66abd6809",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.13MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.10MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.87MB/s]\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "408554a0ea0a7011"
      },
      "cell_type": "markdown",
      "source": [
        "## Phase Encoding Setup\n",
        "\n",
        "#### Phase Encoding Parameters:\n",
        "- These variables (`N`, `omega_active`, `theta_thresh`, `omega_ref`, `n`, `kappa, x`) are parameters used in the phase encoding process.\n",
        "- They control aspects of how the image data is transformed into a phase-encoded representation.\n",
        "\n",
        "#### Input:\n",
        "\n",
        "- $I$: Flattened image (a vector of pixel values)\n",
        "- $N$: The X 8 Y dimensions of the image = N pixel count\n",
        "- $\\omega_{active}$: Active frequency parameter\n",
        "- $x$: Spatial layout parameter (a vector) initialized to a range of values representing a phase gradient across a field of sensory neurons.\n",
        "\n",
        "#### Parameters:\n",
        "\n",
        "- $\\theta_{thresh}$: Threshold phase (set to `0.0` in the code). When a Neuron's phase has rotated through $2\\pi$ to $0$ we consider the Neuron has generated a Spike.\n",
        "- $\\omega_{ref}$: Reference frequency (set to `20 Hz` in the code).\n",
        "- $n$: Scale factor (set to `4.0` in the code) that controls the spread of the reference phase calculation. Higher values compress the phase distribution.\n",
        "- $\\kappa$: Phase gradient coefficient (set to $2\\pi$ in the code) that controls how strongly the spatial layout affects the reference phase calculation.\n"
      ],
      "id": "408554a0ea0a7011"
    },
    {
      "metadata": {
        "id": "4eab0a36bfc3d402"
      },
      "cell_type": "code",
      "source": [
        "# --- 3. Phase Encoding Parameters ---\n",
        "N = 28 * 28\n",
        "omega_active = torch.ones(N, dtype=torch.float32) * 2 * np.pi * 20.0\n",
        "theta_thresh = 0.0\n",
        "omega_ref = 2 * np.pi * 8.0\n",
        "n = 4.0\n",
        "kappa = 2 * np.pi\n",
        "x_spatial_field = torch.linspace(0, 1, N, dtype=torch.float32)\n",
        "\n",
        "EncoderFnType = Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]"
      ],
      "id": "4eab0a36bfc3d402",
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "2b680036b2e3b9c"
      },
      "cell_type": "markdown",
      "source": [
        "### Phase Encoder Function\n",
        "\n",
        "#### Calculation Steps:\n",
        "\n",
        "1. **Initial Phase** ($\\theta_{init}$) - Scales input pixel values (0-1) to phase range (0-2π): $$\\theta_{init} = I \\cdot 2\\pi$$\n",
        "\n",
        "2. **Phase Difference to Threshold** ($\\Delta\\theta$) - Calculates how far each initial phase needs to rotate to reach threshold: $$\\Delta\\theta = (\\theta_{thresh} - \\theta_{init} + 2\\pi) \\pmod{2\\pi}$$\n",
        "\n",
        "3. **Spike Time** ($t_{spike}$) - Converts phase difference into time domain using angular velocity: $$t_{spike} = \\frac{\\Delta\\theta}{\\omega_{active}}$$\n",
        "\n",
        "4. **Reference Phase** ($\\theta_{ref}$):\n",
        "- Combines temporal coding (spike time) with spatial information $(x)$\n",
        "- Normalized by factor n to control phase distribution: $$\\theta_{ref} = \\left(\\frac{\\omega_{ref} \\cdot t_{spike} + \\kappa \\cdot x}{n}\\right) \\pmod{2\\pi}$$\n",
        "\n",
        "5. **Final Phase Difference** ($\\phi$) - Computes phase difference between reference and threshold: $$\\phi = (\\theta_{thresh} - \\theta_{ref} + 2\\pi) \\pmod{2\\pi}$$\n",
        "\n",
        "#### Output Encoding:\n",
        "Final phase is encoded as a 2D vector using: $$[\\cos(\\phi), \\sin(\\phi)]$$\n",
        "This representation:\n",
        "- Preserves circular nature of phase\n",
        "- Provides continuous, differentiable values\n",
        "- Maintains equal magnitude across all phases\n",
        "\n",
        "### Formal Definition:\n",
        "$$ \\text{encode\\_image}(I, \\omega_{active}, x) = [\\cos(\\phi), \\sin(\\phi)] $$\n",
        "where\n",
        "$$ \\phi = (\\theta_{thresh} - \\theta_{ref} + 2\\pi) \\pmod{2\\pi} $$\n"
      ],
      "id": "2b680036b2e3b9c"
    },
    {
      "metadata": {
        "id": "a4c761e22a93107d"
      },
      "cell_type": "code",
      "source": [
        "# --- 4. Phase Encoder Function ---\n",
        "def encode_image(img_batch: torch.Tensor,\n",
        "                 omega_active_param: torch.Tensor,\n",
        "                 x_param: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Vectorized phase encoder that processes a batch of flattened images at once.\n",
        "\n",
        "    Args:\n",
        "      img_batch: A tensor of shape (B, N) where B is the batch size and N is the\n",
        "                 flattened image dimension.\n",
        "      omega_active_param: A tensor of shape (N,) representing the active frequency.\n",
        "      x_param: A tensor of shape (N,) representing the spatial layout.\n",
        "\n",
        "    Returns:\n",
        "      A tensor of shape (B, 2*N) which is the concatenation of cosine and sine encoded values.\n",
        "    \"\"\"\n",
        "    theta_init = img_batch * 2 * np.pi\n",
        "    delta_theta = torch.fmod(theta_thresh - theta_init + 2 * np.pi, 2 * np.pi)\n",
        "    t_spike = delta_theta / omega_active_param  # Broadcasting over batch dimension\n",
        "    theta_ref = torch.fmod((omega_ref * t_spike + kappa * x_param) / n, 2 * np.pi)\n",
        "    phase_diff = torch.fmod(theta_thresh - theta_ref + 2 * np.pi, 2 * np.pi)\n",
        "    # Concatenate along the feature dimension\n",
        "    return torch.cat([torch.cos(phase_diff), torch.sin(phase_diff)], dim=1)"
      ],
      "id": "a4c761e22a93107d",
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "f56bf025049ca8ef"
      },
      "cell_type": "code",
      "source": [
        "# --- 5. Define Classifier ---\n",
        "class PhaseClassifier(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super(PhaseClassifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(N * 2, L1_OUTPUT_DIMS)\n",
        "        self.layer2 = nn.Linear(L1_OUTPUT_DIMS, L2_OUTPUT_DIMS)\n",
        "        self.layer3 = nn.Linear(L2_OUTPUT_DIMS, num_classes)\n",
        "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Xavier/Glorot initialization for weights\n",
        "        nn.init.xavier_uniform_(self.layer1.weight)\n",
        "        nn.init.xavier_uniform_(self.layer2.weight)\n",
        "        nn.init.xavier_uniform_(self.layer3.weight)\n",
        "\n",
        "        # Initialize biases with a small constant\n",
        "        nn.init.constant_(self.layer1.bias, 0.01)\n",
        "        nn.init.constant_(self.layer2.bias, 0.01)\n",
        "        nn.init.constant_(self.layer3.bias, 0.01)\n",
        "\n",
        "    def forward(self,\n",
        "                batch: torch.Tensor,\n",
        "                omega_active_param: torch.Tensor,\n",
        "                spatial_layout: torch.Tensor,\n",
        "                encoder_fn: Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor] = encode_image\n",
        "                ):\n",
        "        # Flatten the batch of images: (B, 1, H, W) -> (B, N)\n",
        "        batch_flat = batch.view(batch.size(0), -1)\n",
        "\n",
        "        # Vectorized encoding of the entire batch\n",
        "        encoded_images = encoder_fn(batch_flat, omega_active_param, spatial_layout)\n",
        "\n",
        "        encoded_images = self.dropout(encoded_images) # Apply dropout at sensory input level\n",
        "        x = self.layer1(encoded_images)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu(x)\n",
        "        logits = self.layer3(x)\n",
        "        return logits\n"
      ],
      "id": "f56bf025049ca8ef",
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "77029a276db7cb34"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Setup\n",
        "- Blah"
      ],
      "id": "77029a276db7cb34"
    },
    {
      "metadata": {
        "id": "3dd0f9cd14202664"
      },
      "cell_type": "code",
      "source": [
        "# --- 6.1 Training Loop ---\n",
        "def run_training(train_loader_param: DataLoader,\n",
        "                 val_loader_param: DataLoader,\n",
        "                 encoder_fn: EncoderFnType = encode_image) -> nn.Module:\n",
        "    num_classes = NUM_CLASSES\n",
        "    _model = PhaseClassifier(num_classes).to(device)\n",
        "    optimizer = optim.Adam(_model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(_model)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = PATIENCE\n",
        "    counter = 0\n",
        "    num_epochs = NUM_EPOCHS\n",
        "\n",
        "    # Move static tensors to device outside loop\n",
        "    omega_dev = omega_active.to(device)\n",
        "    x_dev = x_spatial_field.to(device)\n",
        "\n",
        "    df = pd.DataFrame({'Epoch': [], 'Loss': [], 'Validation Loss': []})\n",
        "    for epoch in range(num_epochs):\n",
        "        _model.train()\n",
        "        train_loss = 0.0\n",
        "        loop = tqdm(train_loader_param, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
        "        for images, labels in loop:\n",
        "            images_dev, labels_dev = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass (batch encoding is performed inside the model)\n",
        "            outputs = _model(images_dev, omega_dev, x_dev, encoder_fn)\n",
        "            loss = criterion(outputs, labels_dev)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Compute average training loss over the epoch\n",
        "        avg_train_loss = train_loss / len(train_loader_param)\n",
        "\n",
        "        # --- Validation ---\n",
        "        _model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader_param:\n",
        "                val_images_dev, val_labels_dev = images.to(device), labels.to(device)\n",
        "                outputs = _model(val_images_dev, omega_dev, x_dev, encoder_fn)\n",
        "                loss = criterion(outputs, val_labels_dev)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader_param)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "        new_row = pd.DataFrame([{'Epoch': epoch + 1, 'Loss': avg_train_loss, 'Validation Loss': avg_val_loss}])\n",
        "        df = pd.concat([df, new_row], ignore_index=True)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping after {epoch + 1} epochs.\")\n",
        "                break\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    fig = px.line(df, x='Epoch', y=['Loss', 'Validation Loss'], title='Training Losses Over Epochs')\n",
        "    fig.show()\n",
        "    return _model"
      ],
      "id": "3dd0f9cd14202664",
      "outputs": [],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "fbc121e30a2defb3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# --- 7. Evaluation ---\n",
        "def run_evaluation(trained_model: nn.Module, test_loader_param: DataLoader,\n",
        "                   encoder_fn: EncoderFnType = encode_image) -> None:\n",
        "    trained_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    omega_active_dev = omega_active.to(device)\n",
        "    x_spatial_field_dev = x_spatial_field.to(device)\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_param:\n",
        "            images_dev, labels_dev = images.to(device), labels.to(device)\n",
        "            outputs: torch.Tensor = trained_model(images_dev, omega_active_dev, x_spatial_field_dev, encoder_fn)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels_dev.size(0)\n",
        "            correct += (predicted == labels_dev).sum().item()\n",
        "\n",
        "            all_labels.extend(labels_dev.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n",
        "\n",
        "    confusion_df = pd.DataFrame(\n",
        "        confusion_matrix(all_labels, all_predictions),\n",
        "        index=[f'True {i}' for i in range(NUM_CLASSES)],\n",
        "        columns=[f'Pred {i}' for i in range(NUM_CLASSES)]\n",
        "    )\n",
        "    fig = px.imshow(confusion_df,\n",
        "                    labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                    title=\"Confusion Matrix\",\n",
        "                    aspect=\"equal\")\n",
        "    fig.show()\n"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "15afca4734b83487"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Testing the Model\n",
        "Let's hook it all up!"
      ],
      "id": "15afca4734b83487"
    },
    {
      "metadata": {
        "id": "6d7068c4149b763b",
        "outputId": "66486595-df10-493a-d01a-0e82fdba36ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model = run_training(train_loader, val_loader, encode_image)\n",
        "run_evaluation(model, test_loader, encode_image)"
      ],
      "id": "6d7068c4149b763b",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PhaseClassifier(\n",
            "  (layer1): Linear(in_features=1568, out_features=784, bias=True)\n",
            "  (layer2): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (layer3): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 1.8875, Validation Loss: 1.4843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Train Loss: 1.5826, Validation Loss: 1.2386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Train Loss: 1.4060, Validation Loss: 1.0497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Train Loss: 1.3002, Validation Loss: 0.8824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Train Loss: 1.2339, Validation Loss: 0.8216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Train Loss: 1.1683, Validation Loss: 0.7664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Train Loss: 1.1281, Validation Loss: 0.6936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Train Loss: 1.0904, Validation Loss: 0.6748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Train Loss: 1.0689, Validation Loss: 0.7592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Train Loss: 1.0381, Validation Loss: 0.6234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Train Loss: 1.0198, Validation Loss: 0.5963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Train Loss: 0.9997, Validation Loss: 0.5660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Train Loss: 0.9862, Validation Loss: 0.5848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c65915d81540a464"
      },
      "cell_type": "markdown",
      "source": [
        "## Ablation Study:\n",
        "\n",
        "We'll create a modified version of the code where we remove the `t_spike` calculation and directly encode the pixel values using cosine and sine.\n",
        "\n",
        "Comparing the performance of this modified version to our original model should further evidence the importance of the first spike time calculation."
      ],
      "id": "c65915d81540a464"
    },
    {
      "metadata": {
        "id": "7934e8f787b3768a"
      },
      "cell_type": "code",
      "source": [
        "# noinspection PyUnusedLocal\n",
        "def direct_encode(img_batch: torch.Tensor,\n",
        "                  omega_active_ignored: torch.Tensor,\n",
        "                  spatial_layout_ignored: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Directly encodes a batch of flattened images using cosine and sine functions.\n",
        "\n",
        "    This implementation scales the pixel values of each image to [0, 2*pi] and computes\n",
        "    the cosine and sine to produce a tensor of shape (B, 2*N).\n",
        "\n",
        "    Args:\n",
        "      img_batch: A tensor of shape (B, N) where B is the batch size.\n",
        "      omega_active_ignored: Not used.\n",
        "      spatial_layout_ignored: Not used.\n",
        "\n",
        "    Returns:\n",
        "      A tensor of shape (B, 2*N) with the encoded representations.\n",
        "    \"\"\"\n",
        "    scaled_pixels = img_batch * 2 * torch.pi\n",
        "    cos_encoding = torch.cos(scaled_pixels)\n",
        "    sin_encoding = torch.sin(scaled_pixels)\n",
        "    return torch.cat([cos_encoding, sin_encoding], dim=1)"
      ],
      "id": "7934e8f787b3768a",
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "d1836a6f9af16e16"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Testing the Default Encoder\n"
      ],
      "id": "d1836a6f9af16e16"
    },
    {
      "metadata": {
        "id": "5bab47d1c9c4dee5",
        "outputId": "bd599d48-94dc-4c42-b628-59782f54d7d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "wrapped_default_encode = partial(direct_encode)\n",
        "model = run_training(train_loader, val_loader, wrapped_default_encode)\n",
        "run_evaluation(model, test_loader, wrapped_default_encode)"
      ],
      "id": "5bab47d1c9c4dee5",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f49b9c50-9a71-48c1-b5c2-950ed0c0e733\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f49b9c50-9a71-48c1-b5c2-950ed0c0e733\")) {                    Plotly.newPlot(                        \"f49b9c50-9a71-48c1-b5c2-950ed0c0e733\",                        [{\"hovertemplate\":\"variable=Loss\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Loss\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Loss\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0],\"xaxis\":\"x\",\"y\":[1.783571688334147,1.401841784874598,1.234068994442622,1.1448313631216684,1.0914985206921894,1.0444013154506684,1.0088005135059357,0.9928078470230103,0.9804405248959859,0.9620949477354686,0.940706755956014,0.9351904324690501,0.9212155170838038,0.9067383035024007,0.9010908205509186,0.9025128457546234,0.89077139321963,0.8816211658318838,0.8755910221735637,0.8672395518620809],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=Validation Loss\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Validation Loss\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Validation Loss\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0],\"xaxis\":\"x\",\"y\":[1.2855941222069112,0.9460852444171906,0.8196111162926288,0.7879933286854561,0.7258446929936714,0.6554240624004222,0.626030998344117,0.6253653750457662,0.5645428161988867,0.5889019528602032,0.5398312835617268,0.556789092402509,0.5119000010667963,0.5448338590720867,0.49617600987883326,0.5181916891894442,0.5092022926249402,0.49555455362226103,0.4942630199675864,0.47717643338949123],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Training Losses Over Epochs\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f49b9c50-9a71-48c1-b5c2-950ed0c0e733');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 90.80%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"eb9a9077-d4eb-4c04-b892-754f2719a277\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"eb9a9077-d4eb-4c04-b892-754f2719a277\")) {                    Plotly.newPlot(                        \"eb9a9077-d4eb-4c04-b892-754f2719a277\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"Pred 0\",\"Pred 1\",\"Pred 2\",\"Pred 3\",\"Pred 4\",\"Pred 5\",\"Pred 6\",\"Pred 7\",\"Pred 8\",\"Pred 9\"],\"y\":[\"True 0\",\"True 1\",\"True 2\",\"True 3\",\"True 4\",\"True 5\",\"True 6\",\"True 7\",\"True 8\",\"True 9\"],\"z\":[[961,1,0,0,2,1,7,3,3,2],[0,1119,3,0,0,3,3,0,4,3],[13,8,937,5,21,2,5,21,12,8],[0,7,25,869,1,34,0,11,49,14],[1,4,2,0,893,3,13,2,10,54],[5,5,0,18,5,797,41,1,17,3],[3,7,1,0,11,13,922,0,0,1],[4,28,25,2,16,1,1,830,1,120],[5,6,5,5,21,11,9,3,883,26],[6,8,1,5,96,4,2,7,11,869]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Predicted Label: %{x}\\u003cbr\\u003eTrue Label: %{y}\\u003cbr\\u003eCount: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Predicted Label\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"True Label\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Count\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"title\":{\"text\":\"Confusion Matrix\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('eb9a9077-d4eb-4c04-b892-754f2719a277');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 13
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}