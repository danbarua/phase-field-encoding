{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neuromorphic Software: Phase-Encoding\n",
    "\n",
    "Phase encoding is a biologically-inspired method of converting input data (like images) into temporal spike patterns, similar to how biological neurons process information. In neuromorphic computing, it plays a crucial role by:\n",
    "\n",
    "1. Converting static input (pixel values) into dynamic temporal patterns\n",
    "2. Mimicking the way biological sensory neurons encode information through spike timing\n",
    "3. Enabling efficient processing of information in neuromorphic hardware\n",
    "\n",
    "This implementation combines:\n",
    "- A fixed bio-inspired phase encoder that transforms input data into spike-timing based representations\n",
    "- A learnable MultiLayerPerceptron for classification that processes these phase-encoded patterns\n",
    "\n",
    "The encoder uses phase differences and spike timing to create a rich representation of the input data, while preserving spatial relationships through a reference phase calculation. This makes it particularly suitable for pattern recognition tasks like digit classification.\n"
   ],
   "id": "16bb457ff771456f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notebook setup\n",
    "\n",
    "Imports, Random Seeding, Device Setup - Training time varies based on hardware: approximately 20 minutes on CPU, and significantly faster on GPU.\n"
   ],
   "id": "931f06d812fbd91e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython import display\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ],
   "id": "caedfd34a31e6d18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# to ensure reproducibility, we should set the random seed consistently\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)  # for multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ],
   "id": "25d28495c380b35d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "dd3e83c212d88c55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment Setup\n",
    "\n",
    "This section defines key parameters for the experiment:\n",
    "\n",
    "- Classification task parameters (number of classes, data augmentation limits)\n",
    "- Neural network architecture parameters (layer dimensions, dropout rate)\n",
    "- Training hyperparameters (epochs, batch size, learning rate, early stopping patience)\n",
    "\n",
    "These parameters control the behavior of both the phase encoder and the classifier training process.\n"
   ],
   "id": "7a68621a2afb617d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 0.0 Define Experiment Parameters ---\n",
    "NUM_CLASSES = 10  # 10 digits in MNIST Dataset\n",
    "MAX_ROTATION_DEGREES = 35  # Rotate images by up to 35 degrees\n",
    "MAX_TRANSLATION = 0.1  # Translate images by up to 10%\n",
    "\n",
    "# --- 0.1 Define Classifier Model Parameters ---\n",
    "L1_OUTPUT_DIMS = 784\n",
    "L2_OUTPUT_DIMS = 512\n",
    "DROPOUT_PROB = 0.5  # Randomly \"mute\" a proportion of input neurons\n",
    "\n",
    "# --- 0.2 Define Training Parameters ---\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 3"
   ],
   "id": "c6464a56d93cee52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading and Preprocessing MNIST Data\n",
    "- Defines image transformations to be applied to the training and testing data, respectively.\n",
    "- The training data is augmented with random rotations and translations.\n",
    "- Both datasets are converted to PyTorch tensors and normalized.\n",
    "- Normalization is crucial as it helps neural networks converge faster and perform better by ensuring all input features are on a similar scale and centered around zero.\n",
    "- Loads the MNIST dataset.\n",
    "- The root argument specifies where to store the data, train=True indicates the training set, and download=True downloads the data if it's not already present.\n",
    "- Splits the training data into training and validation sets.\n",
    "- This is important to evaluate the model's performance during training and prevent overfitting.\n",
    "- Creates data loaders for the training, validation, and testing sets.\n",
    "- These loaders handle batching and shuffling of the data, making it easier to feed into the model during training and evaluation.\n"
   ],
   "id": "e0b8cac91851e62b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 1. Load and Normalize MNIST ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(MAX_ROTATION_DEGREES),\n",
    "    transforms.RandomAffine(degrees=0, translate=(MAX_TRANSLATION, MAX_TRANSLATION)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation for MNIST\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "# --- Split training data into training and validation sets ---\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# --- 2. Dataloaders ---\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "32df13d66abd6809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Phase Encoding Setup\n",
    "\n",
    "#### Phase Encoding Parameters:\n",
    "- These variables (`N`, `omega_active`, `theta_thresh`, `omega_ref`, `n`, `kappa, x`) are parameters used in the phase encoding process.\n",
    "- They control aspects of how the image data is transformed into a phase-encoded representation.\n",
    "\n",
    "#### Input:\n",
    "\n",
    "- $I$: Flattened image (a vector of pixel values)\n",
    "- $N$: The X 8 Y dimensions of the image = N pixel count\n",
    "- $\\omega_{active}$: Active frequency parameter\n",
    "- $x$: Spatial layout parameter (a vector) initialized to a range of values representing a phase gradient across a field of sensory neurons.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- $\\theta_{thresh}$: Threshold phase (set to `0.0` in the code). When a Neuron's phase has rotated through $2\\pi$ to $0$ we consider the Neuron has generated a Spike.\n",
    "- $\\omega_{ref}$: Reference frequency (set to `20 Hz` in the code).\n",
    "- $n$: Scale factor (set to `4.0` in the code) that controls the spread of the reference phase calculation. Higher values compress the phase distribution.\n",
    "- $\\kappa$: Phase gradient coefficient (set to $2\\pi$ in the code) that controls how strongly the spatial layout affects the reference phase calculation.\n"
   ],
   "id": "408554a0ea0a7011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3. Phase Encoding Parameters ---\n",
    "N = 28 * 28\n",
    "omega_active = torch.ones(N, dtype=torch.float32) * 2 * np.pi * 20.0\n",
    "theta_thresh = 0.0\n",
    "omega_ref = 2 * np.pi * 8.0\n",
    "n = 4.0\n",
    "kappa = 2 * np.pi\n",
    "x_spatial_field = torch.linspace(0, 1, N, dtype=torch.float32)\n",
    "\n",
    "EncoderFnType = Callable[[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]"
   ],
   "id": "4eab0a36bfc3d402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Phase Encoder Function\n",
    "\n",
    "#### Calculation Steps:\n",
    "\n",
    "1. **Initial Phase** ($\\theta_{init}$) - Scales input pixel values (0-1) to phase range (0-2Ï€): $$\\theta_{init} = I \\cdot 2\\pi$$\n",
    "\n",
    "2. **Phase Difference to Threshold** ($\\Delta\\theta$) - Calculates how far each initial phase needs to rotate to reach threshold: $$\\Delta\\theta = (\\theta_{thresh} - \\theta_{init} + 2\\pi) \\pmod{2\\pi}$$\n",
    "\n",
    "3. **Spike Time** ($t_{spike}$) - Converts phase difference into time domain using angular velocity: $$t_{spike} = \\frac{\\Delta\\theta}{\\omega_{active}}$$\n",
    "\n",
    "4. **Reference Phase** ($\\theta_{ref}$):\n",
    "- Combines temporal coding (spike time) with spatial information $(x)$\n",
    "- Normalized by factor n to control phase distribution: $$\\theta_{ref} = \\left(\\frac{\\omega_{ref} \\cdot t_{spike} + \\kappa \\cdot x}{n}\\right) \\pmod{2\\pi}$$\n",
    "\n",
    "5. **Final Phase Difference** ($\\phi$) - Computes phase difference between reference and threshold: $$\\phi = (\\theta_{thresh} - \\theta_{ref} + 2\\pi) \\pmod{2\\pi}$$\n",
    "\n",
    "#### Output Encoding:\n",
    "Final phase is encoded as a 2D vector using: $$[\\cos(\\phi), \\sin(\\phi)]$$\n",
    "This representation:\n",
    "- Preserves circular nature of phase\n",
    "- Provides continuous, differentiable values\n",
    "- Maintains equal magnitude across all phases\n",
    "\n",
    "### Formal Definition:\n",
    "$$ \\text{encode\\_image}(I, \\omega_{active}, x) = [\\cos(\\phi), \\sin(\\phi)] $$\n",
    "where\n",
    "$$ \\phi = (\\theta_{thresh} - \\theta_{ref} + 2\\pi) \\pmod{2\\pi} $$\n"
   ],
   "id": "2b680036b2e3b9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4. Phase Encoder Function ---\n",
    "def encode_image(img_flat: torch.Tensor,\n",
    "                 omega_active_param: torch.Tensor,\n",
    "                 x_param: torch.Tensor) -> torch.Tensor:\n",
    "    theta_init = (img_flat * 2 * np.pi)\n",
    "    delta_theta = torch.fmod(theta_thresh - theta_init + 2 * np.pi, 2 * np.pi)\n",
    "    t_spike = delta_theta / omega_active_param\n",
    "    theta_ref = torch.fmod((omega_ref * t_spike + kappa * x_param) / n, 2 * np.pi)\n",
    "    phase_diff = torch.fmod(theta_thresh - theta_ref + 2 * np.pi, 2 * np.pi)\n",
    "    return torch.cat([torch.cos(phase_diff), torch.sin(phase_diff)])"
   ],
   "id": "a4c761e22a93107d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 5. Define Classifier ---\n",
    "class PhaseClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(PhaseClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(N * 2, L1_OUTPUT_DIMS)\n",
    "        self.layer2 = nn.Linear(L1_OUTPUT_DIMS, L2_OUTPUT_DIMS)\n",
    "        self.layer3 = nn.Linear(L2_OUTPUT_DIMS, num_classes)\n",
    "        self.dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Initialize weights using Xavier/Glorot initialization\n",
    "        nn.init.xavier_uniform_(self.layer1.weight)\n",
    "        nn.init.xavier_uniform_(self.layer2.weight)\n",
    "        nn.init.xavier_uniform_(self.layer3.weight)\n",
    "\n",
    "        # Initialize biases to small constant values\n",
    "        nn.init.constant_(self.layer1.bias, 0.01)\n",
    "        nn.init.constant_(self.layer2.bias, 0.01)\n",
    "        nn.init.constant_(self.layer3.bias, 0.01)\n",
    "\n",
    "    def forward(self,\n",
    "                batch: torch.Tensor,\n",
    "                omega_active_param: torch.Tensor,\n",
    "                spatial_layout: torch.Tensor,\n",
    "                encoder_fn: EncoderFnType = encode_image\n",
    "                ):\n",
    "        # Flatten all images in batch at once\n",
    "        batch_flat = batch.view(batch.size(0), -1)\n",
    "\n",
    "        # Apply encoder function to all flattened images\n",
    "        encoded_images = torch.stack([\n",
    "            encoder_fn(img, omega_active_param, spatial_layout)\n",
    "            for img in batch_flat\n",
    "        ])\n",
    "\n",
    "        encoded_images = self.dropout(encoded_images)\n",
    "        x = self.relu(self.layer1(encoded_images))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.layer3(x)\n",
    "        return logits\n"
   ],
   "id": "f56bf025049ca8ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Setup\n",
    "- Blah"
   ],
   "id": "77029a276db7cb34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 6.1 Training Loop ---\n",
    "def run_training(train_loader_param: DataLoader,\n",
    "                 val_loader_param: DataLoader,\n",
    "                 encoder_fn: EncoderFnType = encode_image) -> nn.Module:\n",
    "    num_classes = NUM_CLASSES\n",
    "    _model = PhaseClassifier(num_classes).to(device)\n",
    "    optimizer = optim.Adam(_model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(_model)\n",
    "\n",
    "    # --- Early Stopping Parameters ---\n",
    "    best_val_loss = float('inf')\n",
    "    patience = PATIENCE\n",
    "    counter = 0\n",
    "\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    loss = float('inf')\n",
    "\n",
    "    # Move static tensors to device outside loop\n",
    "    omega_dev = omega_active.to(device)\n",
    "    x_dev = x_spatial_field.to(device)\n",
    "\n",
    "    # Create an empty DataFrame to hold epoch and loss values\n",
    "    df = pd.DataFrame({'Epoch': [], 'Loss': [], 'Validation Loss': []})\n",
    "    epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        _model.train()\n",
    "        loop = tqdm(train_loader_param, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n",
    "        for images, labels in loop:\n",
    "            images_dev, labels_dev = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = _model(images_dev, omega_dev, x_dev, encoder_fn)\n",
    "            loss = criterion(outputs, labels_dev)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tqdm loop\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # --- Validation ---\n",
    "        _model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_param:\n",
    "                val_images_dev, val_labels_dev = images.to(device), labels.to(device)\n",
    "                outputs = _model(val_images_dev, omega_dev, x_dev)\n",
    "                loss = criterion(outputs, val_labels_dev)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # --- Output Progress and capture stats for plotting later ---\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "        new_row = pd.DataFrame([{'Epoch': epoch + 1, 'Loss': loss.item(), 'Validation Loss': val_loss}])\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # --- Finished! Plot the Training Progress ---\n",
    "    display.clear_output(wait=True)\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs.\")\n",
    "    else:\n",
    "        print(f\"Finished training after {epoch + 1} epochs.\")\n",
    "    fig = px.line(df, x='Epoch', y=['Loss', 'Validation Loss'], title='Training Losses Over Epochs')\n",
    "    fig.show()\n",
    "\n",
    "    return _model\n"
   ],
   "id": "3dd0f9cd14202664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# --- 7. Evaluation ---\n",
    "def run_evaluation(trained_model: nn.Module, test_loader_param: DataLoader,\n",
    "                   encoder_fn: EncoderFnType = encode_image) -> None:\n",
    "    trained_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    omega_active_dev = omega_active.to(device)\n",
    "    x_spatial_field_dev = x_spatial_field.to(device)\n",
    "\n",
    "    # Create lists to store actual and predicted labels\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader_param:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs: torch.Tensor = trained_model(images, omega_active_dev, x_spatial_field_dev, encoder_fn)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            # noinspection PyUnresolvedReferences\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store labels and predictions\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n",
    "\n",
    "    # Create confusion matrix\n",
    "    confusion_df = pd.DataFrame(\n",
    "        confusion_matrix(all_labels, all_predictions),\n",
    "        index=[f'True {i}' for i in range(NUM_CLASSES)],\n",
    "        columns=[f'Pred {i}' for i in range(NUM_CLASSES)]\n",
    "    )\n",
    "\n",
    "    # Plot confusion matrix using plotly express\n",
    "    fig = px.imshow(confusion_df,\n",
    "                    labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
    "                    title=\"Confusion Matrix\",\n",
    "                    aspect=\"equal\")\n",
    "    fig.show()\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training and Testing the Model\n",
    "Let's hook it all up!"
   ],
   "id": "15afca4734b83487"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = run_training(train_loader, val_loader, encode_image)\n",
    "run_evaluation(model, test_loader, encode_image)"
   ],
   "id": "6d7068c4149b763b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ablation Study:\n",
    "\n",
    "We'll create a modified version of the code where we remove the `t_spike` calculation and directly encode the pixel values using cosine and sine.\n",
    "\n",
    "Comparing the performance of this modified version to our original model should further evidence the importance of the first spike time calculation."
   ],
   "id": "c65915d81540a464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# noinspection PyUnusedLocal\n",
    "def direct_encode(img_flat: torch.Tensor,\n",
    "                  omega_active_ignored: torch.Tensor,\n",
    "                  spatial_layout_ignored: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Directly encodes pixel values using cosine and sine functions.\n",
    "    \n",
    "    This implementation returns the same dimensions as encode_image calculated\n",
    "    as [cos(scaled_pixels), sin(scaled_pixels)] where scaled_pixels is in the range [0, 2*pi].\n",
    "\n",
    "    Args:\n",
    "      img_flat: A flattened PyTorch tensor representing the image pixels.\n",
    "      omega_active_ignored: Not used in this direct method.\n",
    "      spatial_layout_ignored: Not used in this direct method.\n",
    "\n",
    "    Returns:\n",
    "      A PyTorch tensor with shape (2*N,) corresponding to the encoded image.\n",
    "    \"\"\"\n",
    "    # Scale pixel values to the range [0, 2*pi]\n",
    "    scaled_pixels = img_flat * 2 * torch.pi\n",
    "\n",
    "    # Compute cosine and sine directly for each pixel\n",
    "    cos_encoding = torch.cos(scaled_pixels)\n",
    "    sin_encoding = torch.sin(scaled_pixels)\n",
    "\n",
    "    # Concatenate to produce an encoding with 2*N dimensions\n",
    "    encoded_representation = torch.cat([cos_encoding, sin_encoding])\n",
    "\n",
    "    return encoded_representation"
   ],
   "id": "7934e8f787b3768a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and Testing the Default Encoder\n",
   "id": "d1836a6f9af16e16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "wrapped_default_encode = partial(direct_encode)\n",
    "model = run_training(train_loader, val_loader, wrapped_default_encode)\n",
    "run_evaluation(model, test_loader, wrapped_default_encode)"
   ],
   "id": "5bab47d1c9c4dee5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
